import numpy as np
import pandas as pd
pd.options.mode.chained_assignment = None
from os import path
import hashlib
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

# The Malware files are named Malw1, Malw2,...,Malw200 and Mal_file1, Mal_file2,..., Mal_file400.
# The Benign files are named from Beni1, Beni2,...,Beni504.
# Some Malware and Benign files were corrupted and couldn't be extracted so, those files were deleted.
# The Below lists (Beni_nos,Malw_nos,Mal_file_nos) contain the numbers of available files. This would make it easier for us to access them later.

beni_nos=[]
beni_not_nos=[]
Mal_file_nos=[]
Mal_file_not_nos=[]
Malw_nos=[]
Malw_not_nos=[]

for i in range(1,201):
    if ((path.exists('Malw_'+str(i)+'.csv'))):
        Malw_nos.append(i)
    else:
        Malw_not_nos.append(i)            

for i in range(1,401):
    if ((path.exists('Mal_file'+str(i)+'.csv'))):
        Mal_file_nos.append(i)
    else:
        Mal_file_not_nos.append(i) 
 for i in range(1,505):
    if ((path.exists('Beni'+str(i)+'.csv'))):
        beni_nos.append(i)
    else:
        beni_not_nos.append(i) 

 # The hash value of each Opcode Sequence is generated and is added to hashset_benign(for Benign files) and hashset_malware(for Malware files).
 # If the hash value generated by an new file is already present in the hash set, then is a duplicate file and it is not added to the Dataset.
 # This will ensure the Dataset consists of unique Opcode sequences.

hashset_benign = set()
hashset_malware=set()
def hash_sent(sent):
    return hashlib.md5(sent).hexdigest()

# The "corpus" contains the Opcode sequence of the files. 
# 'y' is the dependent variable. '1' denotes the file is a Malware and '0' denotes the file is Benign.

corpus=[]
y=[]

#The below two lists are for my reference.
Mal_file_final_nos=[]
Mal_file_dup_nos=[]

# If the file is unique, its Opcode sequence is appended to the 'corpus' and it is classified in 'y'.
# This is done for all the Malware and Benign Files.

for name in Mal_file_nos:
    print("Mal_file " + str(name))
    csvname1="Mal_file"+str(name)+".csv"
    data1=pd.read_csv(csvname1)
    data1.drop(data1.columns[data1.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)
    hex_opcode=(data1["Hex_Opcode"].dropna().tolist())
    hex_opc=" ".join(hex_opcode)
    hashValue = hash_sent(hex_opc.encode('utf-8'))
    if hashValue in hashset_malware:
        Mal_file_dup_nos.append(name)
    else:
        #print(line.strip('\n'))
        corpus.append(hex_opc)
        hashset_malware.add(hashValue)
        Mal_file_final_nos.append(name)
        y.append(1)
        
Malw_final_nos=[]
Malw_dup_nos=[]
for name in Malw_nos:
    print("Malw "+str(name))
    csvname1="Malw_"+str(name)+".csv"
    data1=pd.read_csv(csvname1)
    data1.drop(data1.columns[data1.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)
    hex_opcode=(data1["Hex_Opcode"].dropna().tolist())
    hex_opc=" ".join(hex_opcode)
    hashValue = hash_sent(hex_opc.encode('utf-8'))
    if hashValue in hashset_malware:
        Malw_dup_nos.append(name)
    else:
        #print(line.strip('\n'))
        corpus.append(hex_opc)
        hashset_malware.add(hashValue)
        Malw_final_nos.append(name)
        y.append(1)

        
beni_final_nos=[]
beni_dup_nos=[]
for name in beni_nos:
    print("Benign"+str(name))
    csvname1="Beni"+str(name)+".csv"
    data1=pd.read_csv(csvname1)
    data1.drop(data1.columns[data1.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)
    hex_opcode=(data1["Hex_Opcode"].dropna().tolist())
    hex_opc=" ".join(hex_opcode)
    hashValue = hash_sent(hex_opc.encode('utf-8'))
    if hashValue in hashset_benign:
        beni_dup_nos.append(name)
        continue
    else:
        #print(line.strip('\n'))
        corpus.append(hex_opc)
        hashset_benign.add(hashValue)
        beni_final_nos.append(name)
        y.append(0)
        
# The list 'y' is changed to array.
y=np.array(y) 

# CountVectorizer converts the Opcode sequence to a matrix of token counts
# CountVectorizer creates the Dataset to be fed into the Machine Learning Algorithms.
# The N-Gram size is given as parameter.
# X1 consists of Opcode as Column names(N-Gram=1) and either Opcode Count or Opcode Frequency as its row data.
# X2 consists of Opcodes as Column names(N-Gram=2) and either Opcode Count or Opcode Frequency as its row data.

from sklearn.feature_extraction.text import CountVectorizer
cv1 = CountVectorizer()
X1 = cv1.fit_transform(corpus).toarray()
cv2 = CountVectorizer(ngram_range=(2,2))
X2 = cv2.fit_transform(corpus).toarray()

# X11 is the Dataset consisting of Opcode Count with N-Gram=1 as it's data.
# X21 is the Dataset consisting of Opcode Count with N-Gram=2 as it's data.
# They are split into Training and Test Data(Although they are split directly from X1 and X2).

from sklearn.model_selection import train_test_split

X11_train, X11_test, y11_train, y11_test = train_test_split(X1, y, test_size = 0.25, random_state = 0)
X21_train, X21_test, y21_train, y21_test = train_test_split(X2, y, test_size = 0.25, random_state = 0)

# X1 and X2 are converted into Frequency form (Mentioned in more detail in README.md).
# X12 is the Dataset consisting of Opcode Frequency with N-Gram=1 as it's data.
# X22 is the Dataset consisting of Opcode Frequency with N-Gram=2 as it's data.
# They are split into Training and Test Data(Although they are split directly from v1 and v2).

v1=np.array(X1).astype(np.float32)
for i in range(len(corpus)):
    s=sum(X1[i])
    v1[i]=((X1[i]/s)*100).astype(np.float32)
    
X12_train, X12_test, y12_train, y12_test = train_test_split(v1, y, test_size = 0.25, random_state = 0)

v2=np.array(X2).astype(np.float32)
for i in range(len(corpus)):
    s=sum(X2[i])
    v2[i]=((X2[i]/s)*100).astype(np.float32)

X22_train, X22_test, y22_train, y22_test = train_test_split(v2, y, test_size = 0.25, random_state = 0)

# All the Datasets are fed into various Machine Learning Classification models.
# The Accuracy and the Confusion Matrix is displayed as Output.

print("GB \n")

from sklearn.naive_bayes import GaussianNB
classifierGB11 = GaussianNB()
classifierGB11.fit(X11_train, y11_train)
y11_pred = classifierGB11.predict(X11_test)
cm11 = confusion_matrix(y11_test, y11_pred)
print(cm11)
print(accuracy_score(y11_test, y11_pred))
print('\n')

classifierGB12 = GaussianNB()
classifierGB12.fit(X12_train, y12_train)
y12_pred = classifierGB12.predict(X12_test)
cm12 = confusion_matrix(y12_test, y12_pred)
print(cm12)
print(accuracy_score(y12_test, y12_pred))
print('\n')

classifierGB21 = GaussianNB()
classifierGB21.fit(X21_train, y21_train)
y21_pred = classifierGB21.predict(X21_test)
cm21 = confusion_matrix(y21_test, y21_pred)
print(cm21)
print(accuracy_score(y21_test, y21_pred))
print('\n')

classifierGB22 = GaussianNB()
classifierGB22.fit(X22_train, y22_train)
y22_pred = classifierGB22.predict(X22_test)
cm22 = confusion_matrix(y22_test, y22_pred)
print(cm22)
print(accuracy_score(y22_test, y22_pred))
print('\n')

print("SVC \n")

from sklearn.svm import SVC

classifiersvc11 = SVC(kernel = 'linear', random_state = 0)
classifiersvc11.fit(X11_train, y11_train)
classifiersvc12= SVC(kernel = 'linear', random_state = 0)
classifiersvc12.fit(X12_train, y12_train)
classifiersvc21 = SVC(kernel = 'linear', random_state = 0)
classifiersvc21.fit(X21_train, y21_train)
classifiersvc22 = SVC(kernel = 'linear', random_state = 0)
classifiersvc22.fit(X22_train, y22_train)

ysvc11_pred=classifiersvc11.predict(X11_test)
ysvc12_pred=classifiersvc12.predict(X12_test)
ysvc21_pred=classifiersvc21.predict(X21_test)
ysvc22_pred=classifiersvc22.predict(X22_test)

cm11 = confusion_matrix(y11_test, ysvc11_pred)
print(cm11)
print(accuracy_score(y11_test, ysvc11_pred))
print('\n')
cm12 = confusion_matrix(y12_test, ysvc12_pred)
print(cm12)
print('\n')
print(accuracy_score(y12_test, ysvc12_pred))
cm21 = confusion_matrix(y21_test, ysvc21_pred)
print(cm21)
print('\n')
print(accuracy_score(y21_test, ysvc21_pred))
cm22 = confusion_matrix(y22_test, ysvc22_pred)
print(cm22)
print(accuracy_score(y22_test, ysvc22_pred))
print('\n')

print("RF \n")

from sklearn.ensemble import RandomForestClassifier

classifierrf11 = RandomForestClassifier(n_estimators = 150, criterion = 'entropy', random_state = 0)
classifierrf11.fit(X11_train, y11_train)
classifierrf12= RandomForestClassifier(n_estimators = 150, criterion = 'entropy', random_state = 0)
classifierrf12.fit(X12_train, y12_train)
classifierrf21 = RandomForestClassifier(n_estimators = 150, criterion = 'entropy', random_state = 0)
classifierrf21.fit(X21_train, y21_train)
classifierrf22 = RandomForestClassifier(n_estimators = 150, criterion = 'entropy', random_state = 0)
classifierrf22.fit(X22_train, y22_train)

yrf11_pred=classifierrf11.predict(X11_test)
yrf12_pred=classifierrf12.predict(X12_test)
yrf21_pred=classifierrf21.predict(X21_test)
yrf22_pred=classifierrf22.predict(X22_test)

cm11 = confusion_matrix(y11_test, yrf11_pred)
print(cm11)
print(accuracy_score(y11_test, yrf11_pred))
print('\n')
cm12 = confusion_matrix(y12_test, yrf12_pred)
print(cm12)
print(accuracy_score(y12_test, yrf12_pred))
print('\n')
cm21 = confusion_matrix(y21_test, yrf21_pred)
print(cm21)
print(accuracy_score(y21_test, yrf21_pred))
print('\n')
cm22 = confusion_matrix(y22_test, yrf22_pred)
print(cm22)
print(accuracy_score(y22_test, yrf22_pred))
print('\n')

print("SVM \n")

classifiersvm11 = SVC(kernel = 'rbf', random_state = 0)
classifiersvm11.fit(X11_train, y11_train)
classifiersvm12= SVC(kernel = 'rbf', random_state = 0)
classifiersvm12.fit(X12_train, y12_train)
classifiersvm21 = SVC(kernel = 'rbf', random_state = 0)
classifiersvm21.fit(X21_train, y21_train)
classifiersvm22 = SVC(kernel = 'rbf', random_state = 0)
classifiersvm22.fit(X22_train, y22_train)

ysvm11_pred=classifiersvm11.predict(X11_test)
ysvm12_pred=classifiersvm12.predict(X12_test)
ysvm21_pred=classifiersvm21.predict(X21_test)
ysvm22_pred=classifiersvm22.predict(X22_test)

cm11 = confusion_matrix(y11_test, ysvm11_pred)
print(cm11)
print(accuracy_score(y11_test, ysvm11_pred))
print('\n')
cm12 = confusion_matrix(y12_test, ysvm12_pred)
print(cm12)
print(accuracy_score(y12_test, ysvm12_pred))
print('\n')
cm21 = confusion_matrix(y21_test, ysvm21_pred)
print(cm21)
print(accuracy_score(y21_test, ysvm21_pred))
print('\n')
cm22 = confusion_matrix(y22_test, ysvm22_pred)
print(cm22)
print(accuracy_score(y22_test, ysvm22_pred))
print('\n')

print("DT \n")

from sklearn.tree import DecisionTreeClassifier

classifierdt11 =DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifierdt11.fit(X11_train, y11_train)
classifierdt12= DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifierdt12.fit(X12_train, y12_train)
classifierdt21 = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifierdt21.fit(X21_train, y21_train)
classifierdt22 = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifierdt22.fit(X22_train, y22_train)

ydt11_pred=classifierdt11.predict(X11_test)
ydt12_pred=classifierdt12.predict(X12_test)
ydt21_pred=classifierdt21.predict(X21_test)
ydt22_pred=classifierdt22.predict(X22_test)

cm11 = confusion_matrix(y11_test, ydt11_pred)
print(cm11)
print(accuracy_score(y11_test, ydt11_pred))
print('\n')
cm12 = confusion_matrix(y12_test, ydt12_pred)
print(cm12)
print(accuracy_score(y12_test, ydt12_pred))
print('\n')
cm21 = confusion_matrix(y21_test, ydt21_pred)
print(cm21)
print(accuracy_score(y21_test, ydt21_pred))
print('\n')
cm22 = confusion_matrix(y22_test, ydt22_pred)
print(cm22)
print(accuracy_score(y22_test, ydt22_pred))
print('\n')

print("KNN \n")

from sklearn.neighbors import KNeighborsClassifier

classifierknn11 =KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
classifierknn11.fit(X11_train, y11_train)
classifierknn12= KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
classifierknn12.fit(X12_train, y12_train)
classifierknn21 =KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
classifierknn21.fit(X21_train, y21_train)
classifierknn22 =KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
classifierknn22.fit(X22_train, y22_train)

yknn11_pred=classifierknn11.predict(X11_test)
yknn12_pred=classifierknn12.predict(X12_test)
yknn21_pred=classifierknn21.predict(X21_test)
yknn22_pred=classifierknn22.predict(X22_test)

cm11 = confusion_matrix(y11_test, yknn11_pred)
print(cm11)
print(accuracy_score(y11_test, yknn11_pred))
print('\n')
cm12 = confusion_matrix(y12_test, yknn12_pred)
print(cm12)
print(accuracy_score(y12_test, yknn12_pred))
print('\n')
cm21 = confusion_matrix(y21_test, yknn21_pred)
print(cm21)
print(accuracy_score(y21_test, yknn21_pred))
print('\n')
cm22 = confusion_matrix(y22_test, yknn22_pred)
print(cm22)
print(accuracy_score(y22_test, yknn22_pred))
print('\n')
